{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification\n",
    "\n",
    "In this problem, you will be analyzing the Twitter data we extracted using [this](https://dev.twitter.com/overview/api) api. This time, we extracted the tweets posted by the following six Twitter accounts: `realDonaldTrump, mike_pence, GOP, HillaryClinton, timkaine, TheDemocrats`.\n",
    "\n",
    "For every tweet, we collected two pieces of information:\n",
    "- `screen_name`: the Twitter handle of the user tweeting and\n",
    "- `text`: the content of the tweet.\n",
    "\n",
    "We divided the tweets into two parts - the train and test sets.  The training set contains both the `screen_name` and `text` of each tweet; the test set only contains the `text`.\n",
    "\n",
    "The overarching goal of the problem is to infer the political inclination (whether **R**epublican or **D**emocratic) of the author from the tweet text. The ground truth (i.e., true class labels) are determined from the `screen_name` of the tweet as follows:\n",
    "- **R**: `realDonaldTrump, mike_pence, GOP`\n",
    "- **D**: `HillaryClinton, timkaine, TheDemocrats`\n",
    "\n",
    "We can treat this as a binary classification problem. We'll follow this common structure to tackling this problem:\n",
    "\n",
    "1. **preprocessing**: clean up the raw tweet text using the various functions offered by [the Natural Language Toolkit (`nltk`)](http://www.nltk.org/genindex.html).\n",
    "2. **features**: construct bag-of-words feature vectors.\n",
    "3. **classification**: learn a binary classification model using [`scikit-learn`](http://scikit-learn.org/stable/modules/classes.html). \n",
    "\n",
    "Note that `nltk` supports optional corpora, toy grammars, trained models, etc. For this assignment, you have to manually install the stopwords list and `WordNetLemmatizer`. We'll begin by installing them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### TESTING nltk_download: PASSED 2/2\n",
      "###\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shenx/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\shenx/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\shenx/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import collections\n",
    "import string\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import gzip\n",
    "import csv\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from testing.testing import test\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('punkt')\n",
    "\n",
    "def nltk_download_test(nltk_download):\n",
    "    nltk_download()\n",
    "    try:\n",
    "        lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()\n",
    "        test.true(lemmatizer is not None)\n",
    "        stopwords=nltk.corpus.stopwords.words('english')\n",
    "        test.true(stopwords is not None)\n",
    "    except LookupError:\n",
    "        test.true(False)\n",
    "        \n",
    "@test\n",
    "def nltk_download():\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Text Processing\n",
    "\n",
    "You first task to fill in the following function which processes and tokenizes raw text. The tokens must:\n",
    "\n",
    "1. be in lower case.\n",
    "2. appear in the same order as in the raw text.\n",
    "3. be in their lemmatized form, if one exists. If a word cannot be lemmatized, do not include it in the output.\n",
    "- **For grammatical reasons, documents are going to use different forms of a word, such as organize, organizes, and organizing. Additionally, there are families of derivationally related words with similar meanings, such as democracy, democratic, and democratization. In many situations, it seems as if it would be useful for a search for one of these words to return documents that contain another word in the set. The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form. For instance: **\n",
    "\n",
    "**am, are, is $\\Rightarrow$ be\n",
    "car, cars, car's, cars' $\\Rightarrow$ car **\n",
    "\n",
    "4. **not** contain any characters other than numbers and digits; you should:\n",
    "   1. remove trailing `'s`: `Children's` becomes `children`\n",
    "   2. omit other apostrophes: `don't` becomes `dont`\n",
    "   3. break tokens at other punctuation and/or unicode characters: `word-of-mouth` becomes `word`, `of`, `mouth` \n",
    "5. if the lemmatized form is a stopword, it should not appear in the output\n",
    "6. not include the parts of any t.co urls. Many tweets contain URLs from the domain `t.co`; you should strip all such URLs.\n",
    "\n",
    "If you figure out the right order to perform these operations, solving this problem is much easier.\n",
    "\n",
    "**Stopwords** are words that appear very often in text, usually playing a grammatical role (\"and\", \"a\", etc.). When comparing text similarity, these are not very useful; so we eliminate them at this stage. (NLTK provides us with a list of stopwords for English, which we will use later.)\n",
    "\n",
    "Hints:\n",
    "\n",
    " - `string.punctuation` is a string of all the punctuation symbols\n",
    " - you should use `nltk.word_tokenize()` in your solution\n",
    " - you should break tokens at all characters that are not in `string.ascii_letters` or `string.digits`\n",
    " - test your URL stripping! It's very easy to make a mistake with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### TESTING preprocess: PASSED 13/13\n",
      "###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def preprocess_test(preprocess):\n",
    "    test.equal(preprocess(\"I'm doing well! How about you?\"), ['im', 'doing', 'well', 'how', 'about', 'you'])\n",
    "    test.equal(preprocess(\"Education is the ability to listen to almost anything without losing your temper or your self-confidence.\"),    ['education', 'is', 'the', 'ability', 'to', 'listen', 'to', 'almost', 'anything', 'without', 'losing', 'your', 'temper', 'or', 'your', 'self', 'confidence'])\n",
    "\n",
    "    # Punctuation and space handling\n",
    "    test.equal(preprocess(\" a..a. .a . a.\"), ['a', 'a', 'a', 'a'])\n",
    "    test.equal(preprocess(\"word-of-mouth self-esteem\"), ['word', 'of', 'mouth', 'self', 'esteem'])\n",
    "\n",
    "    # Apostrophe handling\n",
    "    test.equal(preprocess(\"you've\"), ['youve'])\n",
    "    test.equal(preprocess(\"She's\"), ['she'])\n",
    "    test.equal(preprocess(\"Cea'sar\"), ['ceaar']) # You can assume that there are no mid-word \"'s\" substrings.\n",
    "\n",
    "    # Lemmatizer\n",
    "    test.equal(preprocess(\"walks\"), ['walk'])\n",
    "    \n",
    "    # Stopwords\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    test.equal(preprocess(\"I'm doing well! How about you?\", stopwords), ['im', 'well'])\n",
    "    test.equal(preprocess(\"Education is the ability to listen to almost anything without losing your temper or your self-confidence.\", stopwords), ['education', 'ability', 'listen', 'almost', 'anything', 'without', 'losing', 'temper', 'self', 'confidence'])\n",
    "\n",
    "    # Unicode handling\n",
    "    test.equal(preprocess(\"dootüëèdoot\"), [\"doot\", \"doot\"])\n",
    "\n",
    "    # URL handling\n",
    "    test.equal(preprocess(\"http://t.co/WJs5bmRthU,http://t.co/WJs5bmRthU,\"), [])\n",
    "    test.equal(preprocess(\"boohttp://t.co/WJs5bmRthUhello\"), [\"boo\", \"hello\"])\n",
    "\n",
    "@test\n",
    "def preprocess(text, stopwords={}, lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()):\n",
    "    \"\"\" Normalizes case and handles punctuation\n",
    "    \n",
    "    args:\n",
    "        text: str -- raw text\n",
    "        stopwords : Set[str] -- lemmatized tokens to exclude from the output\n",
    "        lemmatizer : Lemmatizer -- an instance of a class implementing the lemmatize() method\n",
    "\n",
    "    Outputs:\n",
    "        list(str): tokenized text\n",
    "    \"\"\"\n",
    "    #Step 1 Define all the prerequisites\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    unicode = string.ascii_letters + string.digits    #valid chars, thus, üëè this type of stuff would not be in this list\n",
    "    urlsToRemove = re.findall(r\"http[s]*://t.co/\\w{10}\", text) #urls needed to be removed\n",
    "    punctuations = string.punctuation\n",
    "    \n",
    "    #let us take care of the url\n",
    "    for url in urlsToRemove:\n",
    "        text = text.replace(url, \" \")\n",
    "\n",
    "    #step 2 is to set all the words to be all lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    #step 3 is to remove all the \"'s\" in the raw text to be null\n",
    "    text=text.replace(\"'s\",\"\")\n",
    "    \n",
    "    #step 4 is to remove all ' \n",
    "    text=text.replace(\"'\",\"\")\n",
    "    \n",
    "    #Step 5 remove all the punctuations\n",
    "    for char in punctuations:\n",
    "        text= text.replace(char,\" \")\n",
    "     \n",
    "    #step 6 remove all the invalid characters \n",
    "    for char in text:\n",
    "        if char not in unicode:\n",
    "            text = text.replace(char, \" \")\n",
    "            \n",
    "    #Step 7, tokenize the texts\n",
    "    word_tokens = word_tokenize(text)\n",
    "    \n",
    "    #Step 7, clean out the stop words\n",
    "    stopWords = set(stopwords)\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stopWords]\n",
    "    \n",
    "    cleanedData = []\n",
    "    for word in filtered_sentence:\n",
    "        #lemmatize the chars \n",
    "        word=lemmatizer.lemmatize(word)\n",
    "        cleanedData.append(word)\n",
    "    \n",
    "    return cleanedData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We give you some code that uses `preprocess` to prepare the data. This should take no more than 6s to run; if it takes longer than that, you need to make your preprocessing function run quicker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### TESTING read_data: PASSED 2/2\n",
      "###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %%timeit\n",
    "## Uncomment the previous line to time your code. Remember to comment it out before uploading your solution.\n",
    "\n",
    "def read_data_test(read_data):\n",
    "    data_train, data_test = read_data()\n",
    "    \n",
    "    test.equal(len(data_train), 17298)\n",
    "    test.equal(len(data_test), 1000)\n",
    "\n",
    "def read_csv(stem, process=lambda x: x):\n",
    "    with gzip.open(f\"{stem}.csv.gz\", \"rt\", newline='', encoding=\"UTF-8\") as file:\n",
    "        csvr = csv.reader(file)\n",
    "        next(csvr)\n",
    "        return list(map(process, csvr))\n",
    "\n",
    "def is_republican(r):\n",
    "    return r in [\"realDonaldTrump\", \"mike_pence\", \"GOP\"]\n",
    "\n",
    "@test\n",
    "def read_data(extra_stopwords=set()):\n",
    "    \"\"\"Reads the dataset from the csv.gz files\n",
    "    \n",
    "    return : Tuple[data_train, data_test]\n",
    "        data_train : List[Tuple[is_republican, tokenized_tweet]]\n",
    "            is_republican : bool -- True if tweet is from a republican\n",
    "            tokenized_tweet : List[str] -- the tweet, tokenized by preprocess()\n",
    "    \"\"\"\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english')) | set([\"http\", \"co\", \"rt\", \"amp\"]) | extra_stopwords\n",
    "    data_train = read_csv(\"tweets_train\", process=lambda r: (is_republican(r[0]), preprocess(r[1], stopwords)))\n",
    "    data_test = read_csv(\"tweets_test\", process=lambda r: preprocess(r[0], stopwords))\n",
    "    \n",
    "    return (data_train, data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Construction\n",
    "\n",
    "The next step is to derive feature vectors from the tokenized tweets. In this section, you will be constructing a bag-of-words [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) feature vector.\n",
    "\n",
    "The number of possible words is prohibitively large, and not all words are useful for our task. We will begin by filtering the vectors using a common heuristic:\n",
    "\n",
    "We calculate a frequency distribution of words in the corpus, and remove words at the head (most frequent) and tail (least frequent) of the distribution. Most frequently used words (often called stopwords) provide very little information about the similarity of two pieces of text; we have already removed these. Words with extremely low frequency tend to be typos.\n",
    "\n",
    "We will now implement a function which counts the number of times that each token is used in the training corpus. You should return a [`collections.Counter`](https://docs.python.org/3/library/collections.html#collections.Counter) object with the number of times that each word appears in the dataset.\n",
    "\n",
    "(This should take no more than 20s to run, including reading the files.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_train, data_test = read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# stopWord = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# vectorizer = TfidfVectorizer(stop_words= stopWord)\n",
    "# wordsCount = collections.Counter()\n",
    "# z = ['blue', 'red', 'blue', 'yellow', 'blue', 'red'] \n",
    "# wordsCount.update(z)\n",
    "# z = ['shit']\n",
    "# wordsCount.update(z)\n",
    "# for count in wordsCount:\n",
    "#     if wordsCount[count] == 1:\n",
    "#         del wordsCount[count]     \n",
    "# wordsCount.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### TESTING get_distribution: PASSED 8/8\n",
      "###\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_distribution_test(get_distribution):\n",
    "    data_train, data_test = read_data()\n",
    "    dist = get_distribution(data_train)\n",
    "    test.true(isinstance(dist, collections.Counter))\n",
    "    if dist is None:\n",
    "        return\n",
    "\n",
    "    test.equal(dist['trump'], 1812)\n",
    "    test.equal(dist['clinton'], 1107)\n",
    "    test.equal(dist['president'], 788)\n",
    "    test.equal(dist['american'], 745)\n",
    "    test.equal(dist['job'], 676)\n",
    "    test.equal(dist['obama'], 438)\n",
    "    test.equal(dist['hoosier'], 393)\n",
    "\n",
    "    plt.hist(dist.values(), bins=100)\n",
    "    plt.yscale('log')\n",
    "\n",
    "@test\n",
    "def get_distribution(data_train):\n",
    "    \"\"\" Calculates the word count distribution, excluding stopwords.\n",
    "\n",
    "    args: \n",
    "        data_train -- the training data\n",
    "\n",
    "    return : collections.Counter -- the distribution of word counts\n",
    "    \"\"\"\n",
    "    #it is like a dictionary where saves the count of the words\n",
    "    wordsCount = collections.Counter()\n",
    "    \n",
    "    for eachBatch in data_train:\n",
    "        txt = eachBatch[1]\n",
    "        wordsCount.update(txt)\n",
    "        \n",
    "    return wordsCount\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the distribution looks exponential, even with a logarithmic y-axis; there are a lot words that appear only once. Lets figure out what these words are so we can eliminate them from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### TESTING get_rare_words: PASSED 3/4\n",
      "# 0\t: Failed: 8054 is not equal to 8048\n",
      "###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_rare_words_test(get_rare_words):\n",
    "    data_train, data_test = read_data()\n",
    "    dist = get_distribution(data_train)\n",
    "    new_stopwords = get_rare_words(dist)\n",
    "\n",
    "    test.equal(len(new_stopwords), 8048)\n",
    "    test.true(\"fugedaboudit\" in new_stopwords)\n",
    "    test.true(\"puppybowl\" in new_stopwords)\n",
    "    test.true(\"rusty\" in new_stopwords)    \n",
    "\n",
    "@test\n",
    "def get_rare_words(dist):\n",
    "    \"\"\"use the word count information from the training data to find more stopwords\n",
    "\n",
    "    args:\n",
    "        dist: collections.Counter -- the output of get_distribution\n",
    "\n",
    "    returns : Set[str] -- a set of all words that appear exactly once in the training data\n",
    "    \"\"\"\n",
    "    rareSet = set()\n",
    "    \n",
    "    for word in dist:\n",
    "        if dist[word] == 1:\n",
    "            rareSet.add(word)\n",
    "    \n",
    "    return rareSet\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we provide a wrapper function to cache the preprocessed data. This helps it not take quite as long to re-run. If you change anything above this cell, re-run this cell to clear the cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "global PREPROCESSED_DATA_CACHE\n",
    "PREPROCESSED_DATA_CACHE = None\n",
    "\n",
    "def get_data():\n",
    "    global PREPROCESSED_DATA_CACHE\n",
    "    if PREPROCESSED_DATA_CACHE is None:\n",
    "        data_train, data_test = read_data()\n",
    "        dist = get_distribution(data_train)\n",
    "        new_stopwords = get_rare_words(dist)\n",
    "        PREPROCESSED_DATA_CACHE = read_data(new_stopwords)\n",
    "\n",
    "    return PREPROCESSED_DATA_CACHE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing\n",
    "\n",
    "Now we have each tweet as a list of words, excluding words with high- and low-frequencies. We want to convert these into a sparse feature matrix, where each row corresponds to a tweet and each column to a possible word. We can use `scikit-learn`'s [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) to do this quite easily.\n",
    "\n",
    "There's a catch, though: `TfidfVectorizer` expects the input to be a string, and (by default) it perfoms its own analyzing. You have to override that behavior by passing in `do_nothing` to the constructor as an optional parameter.\n",
    "\n",
    "Hints:\n",
    "\n",
    " - Read [the documentation](https://scikit-learn.org/stable/modules/feature_extraction.html#customizing-the-vectorizer-classes) carefully, and then this [blog post](http://www.davidsbatista.net/blog/2018/02/28/TfidfVectorizer/) ([mirror](http://archive.is/pVdqE)). You need to pass in `do_nothing` in two locations.\n",
    " - You should use just the training data to `fit` or `fit_transform` the vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### TESTING create_features: PASSED 3/5\n",
      "# 0\t: Failed: <17298x8983 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 161469 stored elements in Compressed Sparse Row format> is not equal to <17298x8714 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 161480 stored elements in Compressed Sparse Row format>\n",
      "# 1\t: Failed: <1000x8983 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 9060 stored elements in Compressed Sparse Row format> is not equal to <1000x8714 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 9037 stored elements in Compressed Sparse Row format>\n",
      "###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Helper function, do not change:\n",
    "def do_nothing(x):\n",
    "    return x\n",
    "\n",
    "def create_features_test(create_features):\n",
    "    train_features, train_labels, test_features = create_features(*get_data())\n",
    "\n",
    "    test.equal(repr(train_features), \"\"\"<17298x8714 sparse matrix of type '<class 'numpy.float64'>'\n",
    "\twith 161480 stored elements in Compressed Sparse Row format>\"\"\")\n",
    "\n",
    "    test.equal(repr(test_features), \"\"\"<1000x8714 sparse matrix of type '<class 'numpy.float64'>'\n",
    "\twith 9037 stored elements in Compressed Sparse Row format>\"\"\")\n",
    "\n",
    "    test.equal(train_labels.dtype, bool)\n",
    "    test.equal(len(train_labels), 17298)\n",
    "    test.equal(sum(train_labels), 8646)\n",
    "\n",
    "@test\n",
    "def create_features(train_data, test_data):\n",
    "    \"\"\"creates the feature matrices and label vector for the training and test sets.\n",
    "\n",
    "    args:\n",
    "        train_data : List[Tuple[is_republican, tweet_words]]\n",
    "            is_republican : bool -- True if Republican, False otherwise\n",
    "            tweet_words : List[str] -- the processed tweet tokens\n",
    "        test_data : List[List[str]] -- a list of processed tweets\n",
    "\n",
    "    returns: Tuple[train_features, train_labels, test_features]\n",
    "        train_features : scipy.sparse.csr.csr_matrix -- feature matrix for the training set\n",
    "        train_labels : np.array[num_train] -- a numpy vector, where 1 stands for Republican and 0 stands for Democrat \n",
    "        test_features : scipy.sparse.csr.csr_matrix -- feature matrix for the test set\n",
    "    \"\"\"\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    vectorizer = TfidfVectorizer(analyzer = 'word', tokenizer = do_nothing, preprocessor = do_nothing, token_pattern = None)\n",
    "    allTweetsInfo = [x[1] for x in train_data]\n",
    "    allLabelBoolean = [x[0] for x in train_data]\n",
    "    \n",
    "    fitter = vectorizer.fit(allTweetsInfo)\n",
    "    train_features, test_features = fitter.transform(allTweetsInfo), fitter.transform(test_data)\n",
    "#     print(train_features)\n",
    "    \n",
    "    train_labels = allLabelBoolean\n",
    "#     print(train_labels)\n",
    "\n",
    "    return (train_features, np.asarray(train_labels), test_features)\n",
    "\n",
    "# train_data, test_data = get_data()\n",
    "# train_features, _, _ = create_features(train_data,test_data)\n",
    "\n",
    "# train_features\n",
    "# train_features.asarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that the created matrices are very sparse.\n",
    "\n",
    "Now that we have the features, lets perform the classification:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classification\n",
    "\n",
    "We are ready to put it all together and train the classification model.\n",
    "\n",
    "You will be will be using the Support Vector Machine [`sklearn.svm.SVC`](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC). [Here](http://docs.opencv.org/2.4/doc/tutorials/ml/introduction_to_svm/introduction_to_svm.html) is a quick introduction to SVMs.\n",
    "\n",
    "At the heart of an SVM is the concept of a _kernel function_, which determines the distance between two data points. `sklearn.svm.SVC` natively supports four kernel functions: `linear`, `poly`, `rbf`, `sigmoid`. For this problem space, we will use the `linear` kernel.\n",
    "\n",
    "In this section, we will:\n",
    "\n",
    "1. build a classifier using the `linear` kernel,\n",
    "2. train it using the training set,\n",
    "3. evaluate the trained model on the training set, and then\n",
    "4. use it to predict classification on our test set.\n",
    "\n",
    "Let's begin by training a classifier. This should take no more than 20s to run. You should set the optional parameter `gamma` to `auto`, but leave the rest at their default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### TESTING learn_classifier: PASSED 1/1\n",
      "###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def learn_classifier_test(learn_classifier):\n",
    "    train_features, train_labels, _ = create_features(*get_data())\n",
    "    classifier = learn_classifier(train_features, train_labels)\n",
    "\n",
    "    test.equal(repr(classifier).replace(\"\\n\", \"\").replace(\"  \", \" \").replace(\"  \", \" \"), \"\"\"SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False)\"\"\")\n",
    "\n",
    "@test\n",
    "def learn_classifier(train_features, train_labels, kernel=\"linear\"):\n",
    "    \"\"\"learns a classifier from the input features and labels using a specified kernel function\n",
    "\n",
    "    args:\n",
    "        train_features: scipy.sparse.csr.csr_matrix -- sparse matrix of features\n",
    "        train_labels : numpy.ndarray(bool): binary vector of class labels\n",
    "        kernel : str -- kernel function to be used with classifier, must be (linear|poly|rbf|sigmoid)\n",
    "\n",
    "    return : sklearn.svm.classes.SVC -- classifier\n",
    "    \"\"\"\n",
    "\n",
    "    assert kernel in [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]\n",
    "    model = sklearn.svm.SVC(kernel = kernel, gamma = 'auto')\n",
    "    modelFitted = model.fit(train_features, train_labels)\n",
    "    return modelFitted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to train a classifier, the next step is to measure its performance. This step is necessary to select the best model among a given set of models, or even tune hyperparameters for a given model.\n",
    "\n",
    "We would ordinarily use a held-out validation set to evaluate the performance of the classifier. The use of a held-out set prevents overfitting to the data, and you will do this for another assignment. For this problem, though, we can use the training set.\n",
    "\n",
    "To measure classification accuracy we will use the [$F_1$ score](https://en.wikipedia.org/wiki/F1_score). Implement this:\n",
    "\n",
    "In statistical analysis of binary classification, the F1 score (also F-score or F-measure) is a measure of a test's accuracy. It considers both the precision p and the recall r of the test to compute the score: **p is the number of correct positive results** divided by **the number of all positive results returned by the classifier**, and r is the **number of correct positive results** divided by the number of **all relevant samples (all samples that should have been identified as positive)**. The F1 score is the harmonic mean of the precision and recall, where an F1 score reaches its best value at 1 (perfect precision and recall) and worst at 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### TESTING f1: PASSED 6/6\n",
      "###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def f1_test(f1):\n",
    "    test.equal(f1([1,1,1], [1,1,1]), 1.0)\n",
    "    test.equal(f1([1,0,0], [1,0,0]), 1.0)\n",
    "    test.equal(f1([1,1,0], [1,1,1]), 0.8)\n",
    "    test.equal(f1([1,0,0], [1,1,0]), 2/3)\n",
    "    test.equal(f1([0,0,1], [1,0,1]), 2/3)\n",
    "    test.equal(f1([1,0,0], [1,1,1]), 0.5)\n",
    "\n",
    "@test\n",
    "def f1(pred, ground):\n",
    "    \"\"\" evaluates a classifier based on a supplied validation data\n",
    "\n",
    "    args:\n",
    "        pred: numpy.ndarray(bool) -- predictions\n",
    "        ground: numpy.ndarray(bool) -- known ground-truth values\n",
    "    \n",
    "    return : double -- the F1 score of the predictions\n",
    "    \"\"\"\n",
    "    pred = np.array(pred, dtype=bool)\n",
    "    ground = np.array(ground, dtype=bool)\n",
    "    return sklearn.metrics.f1_score(ground, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we calculate the F1 score on the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### TESTING evaluate: PASSED 0/1\n",
      "# 0\t: Assertion failed\n",
      "###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate_test(evaluate):\n",
    "    train_features, train_labels, _ = create_features(*get_data())\n",
    "    test.true(np.abs(evaluate(train_features, train_labels, 'linear') - 0.9538984242282234) < 1e-5)\n",
    "\n",
    "@test\n",
    "def evaluate(train_features, train_labels, kernel=\"linear\"):\n",
    "    \"\"\"train the classifier and report the F1 score on the training set\n",
    "    \n",
    "    args:\n",
    "        train_features: scipy.sparse.csr.csr_matrix -- sparse matrix of features\n",
    "        train_labels : numpy.ndarray(bool): binary vector of class labels\n",
    "        kernel : str -- kernel function to be used with classifier, must be (linear|poly|rbf|sigmoid)\n",
    "\n",
    "    return : double -- the F1 score of the predictions on the training labels\n",
    "    \"\"\"\n",
    "    model = learn_classifier(train_features, train_labels)\n",
    "    prediction = model.predict(train_features)\n",
    "    return f1(prediction, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying Test Tweets\n",
    "\n",
    "Home stretch! Now we can classify the test tweets! Use `learn_classifier` to make a trained classifier and predict the labels given the `test_features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td><b>D</b></td><td>A comprehensive look at the many lies and offenses of Donald Trump: https://t.co/HKY6HxxFUX https://t.co/cF5GsywU3f                            </td></tr>\n",
       "<tr><td><b>D</b></td><td>\"I‚Äôm here as a proud American, a proud Democrat, a proud mother, and tonight, in particular, a very, very proud daughter.‚Äù ‚Äî@ChelseaClinton    </td></tr>\n",
       "<tr><td><b>R</b></td><td>Oops! Clinton confuses the Constitution with the Declaration of Independence &amp; backs a constitutional right to life.\n",
       "https://t.co/gG6xbptUyo                                                                                                                                                </td></tr>\n",
       "<tr><td><b>R</b></td><td>Secret Server you need to wipe clean? http://t.co/oHlxKqImWB Get Hillary's Secret Server Wiper today. http://t.co/ANbo9R6Qwt                   </td></tr>\n",
       "<tr><td><b>D</b></td><td>\"My dad ran a union ironworking shop...my mom was his best salesman. My brothers &amp; I pitched in...that's how small family businesses do it\"</td></tr>\n",
       "<tr><td><b>D</b></td><td>Thomas Jefferson loved vanilla ice cream. He brought home a recipe from France, which is now in the @librarycongress #VAisForPresidents        </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### TESTING classify_tweets: PASSED 0/0\n",
      "###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def pp(entries):\n",
    "    from IPython.display import HTML, display\n",
    "    import tabulate\n",
    "\n",
    "    display(HTML(tabulate.tabulate([(f'<b>{\"R\" if isr else \"D\"}</b>', txt[0]) for isr, txt in entries], tablefmt='html')))\n",
    "\n",
    "def classify_tweets_test(classify_tweets):\n",
    "    test_original = read_csv(\"tweets_test\")\n",
    "    train_features, train_labels, test_features = create_features(*get_data())\n",
    "    test_classes = classify_tweets(train_features, train_labels, test_features)\n",
    "\n",
    "    pp([e for i, e in enumerate(zip(test_classes, test_original)) if i in [0, 2, 9, 70, 654, 723]])\n",
    "\n",
    "@test\n",
    "def classify_tweets(train_features, train_labels, test_features):\n",
    "    \"\"\"Train a model and predict class labels for the test set.\n",
    "\n",
    "    args:\n",
    "        train_features: scipy.sparse.csr.csr_matrix -- sparse matrix of features\n",
    "        train_labels : numpy.ndarray(bool): binary vector of class labels\n",
    "        test_features: scipy.sparse.csr.csr_matrix -- sparse matrix of features, test set\n",
    "\n",
    "    return : numpy.ndarray[bool] -- True if the corresponding tweet is predicted to be Republican, False otherwise.\n",
    "    \"\"\"\n",
    "    model = learn_classifier(train_features, train_labels)\n",
    "    prediction = model.predict(test_features)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
